{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "fake_news_analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5hng-wlRDc",
        "outputId": "33d4c965-a977-4f68-aad5-debd4ca98146"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# General libraries.\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# SK-learn libraries for learning.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# SK-learn libraries for evaluation.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# SK-learn library for importing the newsgroup data.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# SK-learn libraries for feature extraction from text.\n",
        "from sklearn.feature_extraction.text import *\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSIq-urNlRDe"
      },
      "source": [
        "# kaggle dataset 1\n",
        "\n",
        "fake_raw_1 = pd.read_csv('https://raw.githubusercontent.com/jacobbarkow/w207-final-project-barkow-laface-meehan-skokowski/main/code/dataset_1/Fake.csv')\n",
        "fake_raw_1['label'] = 'fake'\n",
        "\n",
        "true_raw_1 = pd.read_csv('https://raw.githubusercontent.com/jacobbarkow/w207-final-project-barkow-laface-meehan-skokowski/main/code/dataset_1/True.csv')\n",
        "true_raw_1['label'] = 'true'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfInPe_ZlRDe"
      },
      "source": [
        "# kaggle dataset 2\n",
        "\n",
        "raw_2 = pd.read_csv('https://raw.githubusercontent.com/jacobbarkow/w207-final-project-barkow-laface-meehan-skokowski/main/code/dataset_2/fake_train.csv')\n",
        "raw_2['label'] = np.where(raw_2['label'] == 1, 'true', 'fake')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WGKFwh1lRDf"
      },
      "source": [
        "# dataset 3 - COVID\n",
        "\n",
        "fake_covid = pd.read_csv('https://raw.githubusercontent.com/jacobbarkow/w207-final-project-barkow-laface-meehan-skokowski/main/code/dataset_3/fake_covid_dataset.csv')\n",
        "fake_covid_clean = fake_covid.loc[~fake_covid[\"subcategory\"].str.contains(\"partially false\")][[\"title\", \"text\", \"subcategory\"]]\n",
        "fake_covid_clean[\"label\"] = np.where(fake_covid_clean[\"subcategory\"].str.contains(\"false news\"), \"fake\", \"true\")\n",
        "fake_covid_shuffled = fake_covid_clean.sample(frac=1)[[\"title\", \"text\", \"label\"]].reset_index(drop=True)\n",
        "fake_covid_final = pd.concat([fake_covid_shuffled.loc[fake_covid_shuffled[\"label\"]==\"true\"].head(659), fake_covid_shuffled.loc[fake_covid_shuffled[\"label\"]==\"fake\"]]).dropna().reset_index(drop=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqj_ccsYlRDf",
        "outputId": "020b55db-7dd1-43ce-be78-3d4fce2a4d56"
      },
      "source": [
        "# keep only text, title and label and combine to form to full set\n",
        "\n",
        "full_set = fake_raw_1[['title', 'text', 'label']]\n",
        "full_set = full_set.append(true_raw_1[['title', 'text', 'label']])\n",
        "full_set = full_set.append(raw_2[['title', 'text', 'label']])\n",
        "full_set = full_set.append(fake_covid_final[['title', 'text', 'label']])\n",
        "\n",
        "print(full_set.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(67014, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhThZ3OrlRDf"
      },
      "source": [
        "full_set = shuffle(full_set)\n",
        "\n",
        "train_data = full_set[:52000]\n",
        "test_data = full_set[52000:62000]\n",
        "dev_data = full_set[62000:]\n",
        "\n",
        "train_text, train_title, train_labels = train_data['text'], train_data['title'], train_data['label']\n",
        "test_text, test_title, test_labels = test_data['text'], test_data['title'], test_data['label']\n",
        "dev_text, dev_title, dev_labels = dev_data['text'], dev_data['title'], dev_data['label']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "a6I0CgKtlRDg",
        "outputId": "876bc9a0-1651-44f0-9866-637dfd7e67c8"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19663</th>\n",
              "      <td>SPEAKER SCHEDULED TO Praise Hillary Totally Tr...</td>\n",
              "      <td>A Bernie Sanders supporter who slammed Hillary...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11674</th>\n",
              "      <td>Rory McIlroy Comes Back Early From Golf Rehab ...</td>\n",
              "      <td>Rory McIlroy joined President Donald Trump on ...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12305</th>\n",
              "      <td>German police raid locations linked to Islamis...</td>\n",
              "      <td>BERLIN (Reuters) - German police investigating...</td>\n",
              "      <td>true</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5089</th>\n",
              "      <td>Outdoor Clothing Giant Patagonia Pledging to L...</td>\n",
              "      <td>The CEO of outdoor clothing giant Patagonia is...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12845</th>\n",
              "      <td>WOW! DEM STRATEGIST Bob Beckel Says Wikileaks ...</td>\n",
              "      <td>Is CNN contributor and Democratic Strategist B...</td>\n",
              "      <td>fake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ... label\n",
              "19663  SPEAKER SCHEDULED TO Praise Hillary Totally Tr...  ...  fake\n",
              "11674  Rory McIlroy Comes Back Early From Golf Rehab ...  ...  fake\n",
              "12305  German police raid locations linked to Islamis...  ...  true\n",
              "5089   Outdoor Clothing Giant Patagonia Pledging to L...  ...  fake\n",
              "12845  WOW! DEM STRATEGIST Bob Beckel Says Wikileaks ...  ...  fake\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfjZhPbelRDg",
        "outputId": "5a383e20-0b91-4b58-b9d7-95661d775542"
      },
      "source": [
        "# create a CountVectorizer of the training corpus\n",
        "# the nltk list of stopwords is used, and the word 'reuters' is added to avoid overfitting\n",
        "# porterstemmer is used to tokenize the input as well as the stopwords list\n",
        "# the lambda function is used to cast text to strings (causes errors otherwise)\n",
        "\n",
        "def stem_preprocess(word):\n",
        "    ps = nltk.stem.PorterStemmer()\n",
        "    return ps.stem(word)\n",
        "\n",
        "sw = stopwords.words('english')\n",
        "sw.append('reuters')\n",
        "sw_preprocess = sw.copy()\n",
        "\n",
        "for word in sw:\n",
        "    sw_preprocess.append(stem_preprocess(word))\n",
        "\n",
        "cv_train = CountVectorizer(stop_words=sw_preprocess,\n",
        "                          strip_accents='ascii', \n",
        "                          lowercase=True, \n",
        "                          preprocessor=stem_preprocess)\n",
        "train_text_cv = cv_train.fit_transform(train_text.apply(lambda x: np.str_(x)))\n",
        "test_text_cv = cv_train.transform(test_text.apply(lambda x: np.str_(x)))\n",
        "\n",
        "print(np.shape(train_text_cv))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(52000, 213537)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gymYO9k1m52i"
      },
      "source": [
        "# Decision Trees - Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRAATqY8lRDg",
        "outputId": "12f58ad3-a64c-49c3-de77-c51b56d8917c"
      },
      "source": [
        "# Decision Tree Classifier - Sample\n",
        "\n",
        "print(\"\\n---------------------Decision Trees--------------------\")\n",
        "\n",
        "for depth in [2,3,4,5,10,15,20]:\n",
        "  clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
        "  clf.fit(train_text_cv, train_labels)\n",
        "  test_predicted = clf.predict(test_text_cv)\n",
        "  print(\"Tree depth: \", depth, \"F1 Score: \", metrics.f1_score(test_labels, test_predicted, average=\"weighted\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------Decision Tree--------------------\n",
            "Tree depth:  2 F1 Score:  0.6848124152560653\n",
            "Tree depth:  3 F1 Score:  0.7356219548544219\n",
            "Tree depth:  4 F1 Score:  0.7456380977748216\n",
            "Tree depth:  5 F1 Score:  0.7828158310536464\n",
            "Tree depth:  10 F1 Score:  0.8208322582494911\n",
            "Tree depth:  15 F1 Score:  0.8527800499676458\n",
            "Tree depth:  20 F1 Score:  0.8620249289650428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuAnQinAqi4F"
      },
      "source": [
        "# Random Forest - Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yun8BdIQqmhN",
        "outputId": "f549953f-bbb4-464a-c5b1-b4fd4e1f10b9"
      },
      "source": [
        "print(\"\\n---------------------Random Forests--------------------\")\n",
        "\n",
        "for depth in [2,3,4,5,10,15,20]:\n",
        "  rfc = RandomForestClassifier(max_depth=depth)\n",
        "  rfc.fit(train_text_cv, train_labels)\n",
        "  test_predicted = rfc.predict(test_text_cv)\n",
        "  print(\"Tree depth: \", depth, \"F1 Score: \", metrics.f1_score(test_labels, test_predicted, average=\"weighted\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------Random Forests--------------------\n",
            "Tree depth:  2 F1 Score:  0.572174712139491\n",
            "Tree depth:  3 F1 Score:  0.7489877280229595\n",
            "Tree depth:  4 F1 Score:  0.780011607724576\n",
            "Tree depth:  5 F1 Score:  0.7825628642949174\n",
            "Tree depth:  10 F1 Score:  0.8055055897832193\n",
            "Tree depth:  15 F1 Score:  0.8214493521030416\n",
            "Tree depth:  20 F1 Score:  0.8297816970622229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz_iNUitm9qJ"
      },
      "source": [
        "# Multinomial Naive Bayes - Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYydw9OTnAlt"
      },
      "source": [
        "alphas = [1.0e-10, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]\n",
        "l2s = [0.4,0.5,0.6,0.9,1,1.5,5,10,20]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIbb1WjpnDHe",
        "outputId": "33f9e163-daa6-48bd-d0b5-5f02624b776a"
      },
      "source": [
        "print(\"\\n---------------------Naive Bayes--------------------\")\n",
        "for a in alphas:\n",
        "  nb_model = MultinomialNB(alpha=a)\n",
        "  nb_fit = nb_model.fit(train_text_cv,train_labels)\n",
        "  pred_nb = nb_model.predict(test_text_cv)\n",
        "  f1score = metrics.f1_score(test_labels,pred_nb ,average=\"weighted\")\n",
        "  print(\"Alpha: \", a, \"F1 Score: \",f1score)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------Naive Bayes--------------------\n",
            "Alpha:  1e-10 F1 Score:  0.8582299813084111\n",
            "Alpha:  0.0001 F1 Score:  0.8548427071702348\n",
            "Alpha:  0.001 F1 Score:  0.8544137354028543\n",
            "Alpha:  0.01 F1 Score:  0.8499620437016919\n",
            "Alpha:  0.1 F1 Score:  0.8464707776119172\n",
            "Alpha:  0.5 F1 Score:  0.8407565676132636\n",
            "Alpha:  1.0 F1 Score:  0.8377795686825913\n",
            "Alpha:  2.0 F1 Score:  0.8338646447974721\n",
            "Alpha:  10.0 F1 Score:  0.8198095691961272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO0NjPw6nHWB"
      },
      "source": [
        "# Logistic Regression - Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlLWeZJynKVe",
        "outputId": "b7a4c05f-16d1-4cb9-fff2-5bcc16a4da2e"
      },
      "source": [
        "print(\"\\n---------------------Logistic Regression--------------------\")\n",
        "c=1\n",
        "for c in l2s:\n",
        "  lr_model =LogisticRegression(C=c, solver=\"liblinear\", multi_class=\"auto\")\n",
        "  lr_fit = lr_model.fit(train_text_cv,train_labels)\n",
        "  pred_lr = lr_model.predict(test_text_cv)\n",
        "  f1score = metrics.f1_score(test_labels,pred_lr ,average=\"weighted\")\n",
        "  # Get the weights for each\n",
        "  weights = lr_model.coef_\n",
        "  ssw = np.sum(weights,axis=1)**2 ###axis=1 will print out the sum of squared weights for each topic\n",
        "  print(\"L2 Regularization Strength: \", c, \"F1 Score: \",f1score,\"C Values: \",ssw)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------Logistic Regression--------------------\n",
            "L2 Regularization Strength:  0.4 F1 Score:  0.9224960079283675 C Values:  [9980.13291228]\n",
            "L2 Regularization Strength:  0.5 F1 Score:  0.9211927381477373 C Values:  [10062.11337502]\n",
            "L2 Regularization Strength:  0.6 F1 Score:  0.9210924278922512 C Values:  [6857.49217124]\n",
            "L2 Regularization Strength:  0.9 F1 Score:  0.9178797453414754 C Values:  [8491.37430308]\n",
            "L2 Regularization Strength:  1 F1 Score:  0.9205869857984273 C Values:  [8575.63130901]\n",
            "L2 Regularization Strength:  1.5 F1 Score:  0.916982853739491 C Values:  [12718.78775773]\n",
            "L2 Regularization Strength:  5 F1 Score:  0.9147762256204318 C Values:  [20332.20025108]\n",
            "L2 Regularization Strength:  10 F1 Score:  0.9177851435405319 C Values:  [15031.63123608]\n",
            "L2 Regularization Strength:  20 F1 Score:  0.9202947390443358 C Values:  [8504.703211]\n"
          ]
        }
      ]
    }
  ]
}